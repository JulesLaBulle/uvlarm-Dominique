#!/usr/bin/python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from geometry_msgs.msg import Point
from cv_bridge import CvBridge
import pyrealsense2 as rs
import numpy as np
import math
import cv2
from skimage.measure import label, regionprops

class GhostDetectorNode(Node):
    def __init__(self):
        super().__init__('ghost_detector_node')
        self.detected_ghosts = {}
        self.ghost_id = 1
        self._bridge = CvBridge()

    def initializeROSnode(self):
        # Initialisation de la caméra RealSense
        self.pipeline = rs.pipeline()
        self.config = rs.config()
        self.colorizer = rs.colorizer()
        self.config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        self.config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)

        self.pipeline.start(self.config)
        self.align_to = rs.stream.depth
        self.align = rs.align(self.align_to)

        # Création des publishers
        self.string_publisher = self.create_publisher(String, 'ghost_detection', 10)
        self.marker_publisher = self.create_publisher(Point, '/marker_to_place', 10)

        # Création d'un timer pour traiter les images en continu
        self.timer = self.create_timer(0.1, self.process_frame_callback)

        self.get_logger().info("Nœud de détection de Ghost initialisé !")

    def pixel_to_real_size(self, pixel_size, depth, intrinsics):
        fx = intrinsics.fx
        fy = intrinsics.fy
        return (pixel_size * depth) / fx, (pixel_size * depth) / fy

    def process_frame_callback(self):
        try:
            frames = self.pipeline.wait_for_frames()
            aligned_frames = self.align.process(frames)
            depth_frame = aligned_frames.get_depth_frame()
            aligned_color_frame = aligned_frames.get_color_frame()

            if not depth_frame or not aligned_color_frame:
                return

            # Conversion des images en format numpy
            color_image = np.asanyarray(aligned_color_frame.get_data())
            depth_colormap = np.asanyarray(self.colorizer.colorize(depth_frame).get_data())
            hsv_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)

            # Filtrage des pixels verts
            green_lo, green_hi = np.array([35, 100, 50]), np.array([85, 255, 255])
            green_mask = cv2.inRange(hsv_image, green_lo, green_hi)
            green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_OPEN, np.ones((5, 5), np.uint8))

            # Segmentation et propriétés des régions
            label_img = label(green_mask)
            regions = regionprops(label_img)

            for props in regions:
                if props.area < 30:  # Filtrer les petits objets
                    continue

                minr, minc, maxr, maxc = props.bbox
                x_pixel, y_pixel = int((minc + maxc) / 2), int((minr + maxr) / 2)
                depth = depth_frame.get_distance(x_pixel, y_pixel)

                if depth <= 0 or depth > 2:  # Filtrer les objets hors de portée
                    continue

                height, width = maxr - minr, maxc - minc
                color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics
                real_height_y = self.pixel_to_real_size(height, depth, color_intrin)[0]
                real_width_x = self.pixel_to_real_size(width, depth, color_intrin)[0]

                # Calculer l'épaisseur (thickness)
                thickness = depth_frame.get_distance(x_pixel + 5, y_pixel) - depth_frame.get_distance(x_pixel - 5, y_pixel)
                real_thickness_z = abs(thickness)

                # Filtrer par dimensions réelles
                if not (0.07 - 0.03 <= real_height_y <= 0.07 + 0.03) or \
                   not (0.07 - 0.03 <= real_width_x <= 0.07 + 0.03) or \
                   not (0.01 - 0.05 <= real_thickness_z <= 0.01 + 0.05):
                    continue

                # Conversion des pixels en coordonnées réelles dans le repère caméra
                dx, dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x_pixel, y_pixel], depth)
                ghost_x = depth  # Distance en face
                ghost_y = -dy    # Vers la gauche
                ghost_z = -dz    # Vers le haut

                # Ajouter aux fantômes détectés
                self.detected_ghosts[self.ghost_id] = (ghost_x, ghost_y, ghost_z)

                # Publier un message Point
                point_msg = Point()
                point_msg.x = ghost_x
                point_msg.y = ghost_y
                point_msg.z = ghost_z
                self.marker_publisher.publish(point_msg)

                # Publier un message String
                msg = String()
                msg.data = f"Ghost {self.ghost_id} détecté à (x={ghost_x:.2f}, y={ghost_y:.2f}, z={ghost_z:.2f})"
                self.string_publisher.publish(msg)
                self.get_logger().info(msg.data)

                self.ghost_id += 1

                # Annoter l'image avec le rectangle et les informations
                cv2.rectangle(color_image, (minc, minr), (maxc, maxr), (0, 0, 255), 2)  # Rectangle rouge
                cv2.putText(color_image, f"ID: {self.ghost_id - 1}", (minc, minr - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                cv2.putText(color_image, f"Profondeur: {depth:.2f}m", (minc, maxr + 20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)

            combined_image = np.hstack((color_image, depth_colormap))
            cv2.imshow("Détection de Ghosts", combined_image)
            cv2.waitKey(1)

        except Exception as e:
            self.get_logger().error(f"Erreur lors du traitement des images : {e}")

    def destroy_node(self):
        self.pipeline.stop()
        cv2.destroyAllWindows()
        super().destroy_node()

def main():
    rclpy.init()
    node = GhostDetectorNode()
    node.initializeROSnode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
